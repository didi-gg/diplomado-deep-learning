{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le1mqJqhFIFi"
      },
      "source": [
        "# Ejercicio: Normalización de textos y Bolsa de Palabras\n",
        "\n",
        "* En el ejercicio que se trabajara acontinuacion se tomara como material los articulos de la web \"https://www.elmundotoday.com/\".\n",
        "\n",
        "\n",
        "* Estos artículos se encuentran en un fichero csv que esta en el siguiente link ( [corpus_mundo](https://docs.google.com/spreadsheets/d/19Ewh6xUlquMUb0m8poOnjhH_y5hiumTjL55oIMksTlk/edit?usp=sharing)).\n",
        "\n",
        "\n",
        "* Este CSV esta formado por 3 campos que son:\n",
        "    - Tema\n",
        "    - Título\n",
        "    - Texto\n",
        "    \n",
        "    \n",
        "* El objetivo del ejercicio es Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear  ***Bolsa de Palabras***\n",
        "\n",
        "\n",
        "## 1.- Ejercicios de Nomalización:\n",
        "\n",
        "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
        "<span></span><br><br>\n",
        "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        " <span></span><br><br>       \n",
        "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "\n",
        "## 2.- Ejercicios de BoW:\n",
        "\n",
        "* Aprovechando la normalización realizada anteriormente se pide:\n",
        "\n",
        "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "<span></span><br><br>\n",
        "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
        "<span></span><br><br>   \n",
        "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
        "    \n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-Kx8yaTFIFk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLP_sxZFIFl"
      },
      "source": [
        "## 1.- Ejercicios de Nomalización:\n",
        "\n",
        "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXv3dTcIFIFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b98904-4519-4eaa-ce9d-9aac623ddf3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciudadanos exige la dimisión de Cifuentes el año que viene sin falta,cuando haya elecciones Ciudadanos se ha vuelto a mostrar implacable en su oposición al Gobierno del Partido Popular. Apenas unas semanas después de que saltase el escándalo sobre las presuntas irregularidades del máster de Cristina Cifuentes en la Universidad Rey Juan Carlos,el portavoz regional de Ciudadanos,Ignacio Aguado,ha exigido la dimisión de la presidenta para el año que viene sin falta,en cuanto se celebren las elecciones autonómicas de Madrid. Tras dar por vencido el ultimátum dado al PP para que apoye la creación de una comisión de investigación,Aguado se ha mostrado firme y ha dado un nuevo ultimátum amenazando con dar más ultimátums. “Estamos dispuestos a darles todos los ultimátums que hagan falta,no nos va a temblar el pulso”,ha dicho con contundencia. Para Albert Rivera,líder de la formación,“la situación es insostenible y el año que viene,cuando se celebren elecciones,Cristina Cifuentes se tiene que marchar”. De esta manera,la dirección nacional de Ciudadanos quiere demostrar su compromiso de limpiar de corrupción el partido en el Gobierno. “A no ser que Cifuentes sea la más votada en los comicios,no vamos a permitir que siga en su puesto”,ha declarado con severidad. “Al final esto es una democracia y es el pueblo el que decide,pero no le vamos a pasar ni una”,ha añadido. Desde la formación naranja han amenazado incluso con proponer un candidato alternativo para presidir la Comunidad de Madrid. “Creemos que C. Cifuentes sería la persona adecuada para el puesto porque tiene la preparación necesaria para relevar a la actual presidenta y limpiar Madrid de corrupción”,ha dicho Albert Rivera a la prensa.,,,,,,,,,,,,,,,,,,\r\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "docs_file = 'https://docs.google.com/spreadsheets/d/19Ewh6xUlquMUb0m8poOnjhH_y5hiumTjL55oIMksTlk/export?format=csv'\n",
        "docs_list = list()\n",
        "\n",
        "# Use requests to fetch the content from the URL\n",
        "response = requests.get(docs_file)\n",
        "response.encoding = 'utf-8'  # Set encoding to UTF-8\n",
        "file_txt = response.text\n",
        "\n",
        "# Continue with the rest of the code\n",
        "for line in file_txt.split('\\n'):\n",
        "    line = line.split('||')\n",
        "    docs_list.append(line[1] + ' ' + line[2])\n",
        "\n",
        "docs_list = docs_list[1:]  # Elimino la cabecera del fichero\n",
        "print(docs_list[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1pcjklYFIFl"
      },
      "source": [
        "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PFA2keeFIFl"
      },
      "outputs": [],
      "source": [
        "def tokenization(docs_list):\n",
        "    # TODO\n",
        "    return docs_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYuloW4VFIFm"
      },
      "source": [
        "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9SJFzZkFIFm"
      },
      "outputs": [],
      "source": [
        "def remove_words(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4PJp-dFIFm"
      },
      "source": [
        "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RljqAstKFIFm"
      },
      "outputs": [],
      "source": [
        "def lematization(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDi7av2FIFm"
      },
      "source": [
        "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR6vs1k2FIFm"
      },
      "outputs": [],
      "source": [
        "def filter_words(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbo0gbFoFIFn"
      },
      "source": [
        "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FmzpfIGFIFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06d088f-fa5a-49fe-878c-098cfed25410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont Después del revés recibido por el Gobierno de España tras la puesta en libertad de Carles Puigdemont por parte de la justicia alemana,el juez Pablo Llarena ha decidido esta semana,a instancias del Ejecutivo,que sumará a Oriol Junqueras las condenas que no vaya a cumplir el líder del PDeCAT. El exvicepresidente de Cataluña,que permanece en la prisión madrileña de Estremera desde el pasado dos de noviembre,asumiría por tanto todos los delitos atribuidos a Carles Puigdemont y,de esta manera,el Tribunal Supremo se asegura de que los actos del expresidente catalán durante la última legislatura no quedan impunes,ya que “Junqueras pagará por todos y cada uno de ellos”. Con esta maniobra ideada para burlar la justicia alemana,el líder de Esquerra Republicana se enfrenta a 50 años más de prisión. “Seguiremos adelante aunque a Junqueras le caigan cien años más,nadie nos va a parar”,ha dicho hoy Carles Puigdemont desde Alemania. “Haré lo que tenga que hacer y si Junqueras se tiene que sacrificar por ello,lo asumiré con resignación y determinación”,ha prometido. “Seguim!”,tuiteaba poco después de trascender la decisión de Llarena. Según fuentes anónimas del poder judicial,se está barajando también la posibilidad de añadir a la pena de Oriol Junqueras las condenas que puedan imponerse en el futuro a Iñaki Urdangarin,Rodrigo Rato o Esperanza Aguirre,entre otros,así como la de un delito de robo con fuerza ocurrido hace una semana en Huesca y del que la policía ha sido incapaz de encontrar al culpable.,,,,,,,,,,,,,,,,,,\r\n"
          ]
        }
      ],
      "source": [
        "def normalization(docs_list):\n",
        "    corpus = tokenization(docs_list)\n",
        "    corpus = remove_words(corpus)\n",
        "    corpus = lematization(corpus)\n",
        "    corpus = filter_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "corpus = normalization(docs_list)\n",
        "print(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Ibkq96FIFn"
      },
      "source": [
        "<hr>\n",
        "\n",
        "\n",
        "## 2.- Ejercicios de BoW:\n",
        "\n",
        "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICj5pH9cFIFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78577b8a-862e-4b29-aaee-93daab9d7bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont Después del revés recibido por el Gobierno de España tras la puesta en libertad de Carles Puigdemont por parte de la justicia alemana,el juez Pablo Llarena ha decidido esta semana,a instancias del Ejecutivo,que sumará a Oriol Junqueras las condenas que no vaya a cumplir el líder del PDeCAT. El exvicepresidente de Cataluña,que permanece en la prisión madrileña de Estremera desde el pasado dos de noviembre,asumiría por tanto todos los delitos atribuidos a Carles Puigdemont y,de esta manera,el Tribunal Supremo se asegura de que los actos del expresidente catalán durante la última legislatura no quedan impunes,ya que “Junqueras pagará por todos y cada uno de ellos”. Con esta maniobra ideada para burlar la justicia alemana,el líder de Esquerra Republicana se enfrenta a 50 años más de prisión. “Seguiremos adelante aunque a Junqueras le caigan cien años más,nadie nos va a parar”,ha dicho hoy Carles Puigdemont desde Alemania. “Haré lo que tenga que hacer y si Junqueras se tiene que sacrificar por ello,lo asumiré con resignación y determinación”,ha prometido. “Seguim!”,tuiteaba poco después de trascender la decisión de Llarena. Según fuentes anónimas del poder judicial,se está barajando también la posibilidad de añadir a la pena de Oriol Junqueras las condenas que puedan imponerse en el futuro a Iñaki Urdangarin,Rodrigo Rato o Esperanza Aguirre,entre otros,así como la de un delito de robo con fuerza ocurrido hace una semana en Huesca y del que la policía ha sido incapaz de encontrar al culpable.,,,,,,,,,,,,,,,,,,\r\n"
          ]
        }
      ],
      "source": [
        "def drop_less_frecuency_words(corpus, n):\n",
        "    # TODO\n",
        "    return corpus\n",
        "\n",
        "corpus = drop_less_frecuency_words(corpus, 10)\n",
        "print(corpus[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}