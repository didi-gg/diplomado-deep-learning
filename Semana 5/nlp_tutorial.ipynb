{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Conceptos Básicos de NLP"
      ],
      "metadata": {
        "id": "W4S_FSCcPpS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Configuración del entorno\n",
        "Primero, instalamos las bibliotecas necesarias y cargamos los datos."
      ],
      "metadata": {
        "id": "_HwsblyYPsuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necesarias\n",
        "!pip install nltk spacy\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # Para tokenización\n",
        "nltk.download('stopwords')  # Para stopwords\n",
        "nltk.download('averaged_perceptron_tagger')  # Para POS tagging\n",
        "\n",
        "# Cargar modelo de spaCy para español\n",
        "!python -m spacy download es_core_news_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "zym98NUwP4fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b537fbf2-7ebb-4afc-a564-ed9e31465289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Corpus\n",
        "Un corpus es una colección grande y estructurada de textos. Es la base para entrenar modelos de NLP."
      ],
      "metadata": {
        "id": "TRpdJRuZP_OV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKAZjt_LPZye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6258a7-ddf3-4d68-fefe-9e23a74c9dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1: El procesamiento de lenguaje natural es fascinante.\n",
            "Texto 2: La inteligencia artificial está transformando el mundo.\n",
            "Texto 3: Los modelos de lenguaje son cada vez más avanzados.\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de un corpus simple\n",
        "corpus = [\n",
        "    \"El procesamiento de lenguaje natural es fascinante.\",\n",
        "    \"La inteligencia artificial está transformando el mundo.\",\n",
        "    \"Los modelos de lenguaje son cada vez más avanzados.\"\n",
        "]\n",
        "\n",
        "# Mostrar el corpus\n",
        "for i, texto in enumerate(corpus):\n",
        "    print(f\"Texto {i+1}: {texto}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Tokenización\n",
        "La tokenización divide un texto en unidades más pequeñas, como palabras o símbolos."
      ],
      "metadata": {
        "id": "iLJYYPZPQIf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"¡Hola, mundo! ¿Cómo estás?\"\n",
        "Texto=corpus\n",
        "\n",
        "# Tokenización por palabras\n",
        "for i, texto in enumerate(Texto):\n",
        "  tokens = word_tokenize(Texto[i], language='spanish')\n",
        "  print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "id": "OVKGoPqlQODb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3400cb-9d9c-413a-d53a-2692c792d66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['El', 'procesamiento', 'de', 'lenguaje', 'natural', 'es', 'fascinante', '.']\n",
            "Tokens: ['La', 'inteligencia', 'artificial', 'está', 'transformando', 'el', 'mundo', '.']\n",
            "Tokens: ['Los', 'modelos', 'de', 'lenguaje', 'son', 'cada', 'vez', 'más', 'avanzados', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 Normalización\n",
        "La normalización convierte el texto a un formato estándar, como minúsculas y sin puntuación."
      ],
      "metadata": {
        "id": "3PqH9A5gQRwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir a minúsculas y eliminar puntuación\n",
        "import string\n",
        "corpus = [\n",
        "    \"El procesamiento de lenguaje natural es fascinante.\",\n",
        "    \"La inteligencia artificial está transformando el mundo.\",\n",
        "    \"Los modelos de lenguaje son cada vez más avanzados.\"\n",
        "]\n",
        "texto = \"¡Hola, Mundo! ¿Cómo Estás?\"\n",
        "\n",
        "corpus_lower = [elemento.upper() for elemento in corpus]  # Usando list comprehension para mayor concisión\n",
        "corpus = corpus_lower  # Reemplazar el corpus original con la versión en minúsculas\n",
        "print(corpus)\n",
        "\n",
        "for i, texto in enumerate(corpus):\n",
        "  texto = corpus[i].translate(str.maketrans('', '', '¿?!¡.'))  # Eliminar puntuación\n",
        "  print(\"Texto normalizado:\", texto)"
      ],
      "metadata": {
        "id": "A6FqZcZWQckn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83301ec7-6600-4368-bcf0-a65b8b7800b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EL PROCESAMIENTO DE LENGUAJE NATURAL ES FASCINANTE.', 'LA INTELIGENCIA ARTIFICIAL ESTÁ TRANSFORMANDO EL MUNDO.', 'LOS MODELOS DE LENGUAJE SON CADA VEZ MÁS AVANZADOS.']\n",
            "Texto normalizado: EL PROCESAMIENTO DE LENGUAJE NATURAL ES FASCINANTE\n",
            "Texto normalizado: LA INTELIGENCIA ARTIFICIAL ESTÁ TRANSFORMANDO EL MUNDO\n",
            "Texto normalizado: LOS MODELOS DE LENGUAJE SON CADA VEZ MÁS AVANZADOS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.5 Bag of Words (BoW)\n",
        "El bag of words es una representación numérica de un texto, donde se cuenta la frecuencia de cada palabra."
      ],
      "metadata": {
        "id": "EewoJ9ziQe6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Corpus de ejemplo\n",
        "corpus = [\n",
        "    \"El gato está en la casa y el gato es amigo del perro.\",\n",
        "    \"El perro está en el jardín.\",\n",
        "    \"El gato y el perro son amigos.\"\n",
        "]\n",
        "\n",
        "# Crear el modelo Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Mostrar las palabras y sus frecuencias\n",
        "print(\"Palabras:\", vectorizer.get_feature_names_out())\n",
        "print(\"Frecuencias:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "hRUeM1WRQojU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ce5b9c-201d-4fb9-9fc7-9f6f47c6f760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras: ['amigo' 'amigos' 'casa' 'del' 'el' 'en' 'es' 'está' 'gato' 'jardín' 'la'\n",
            " 'perro' 'son']\n",
            "Frecuencias:\n",
            " [[1 0 1 1 2 1 1 1 2 0 1 1 0]\n",
            " [0 0 0 0 2 1 0 1 0 1 0 1 0]\n",
            " [0 1 0 0 2 0 0 0 1 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.6 Stopwords\n",
        "Las stopwords son palabras comunes que no aportan significado (por ejemplo, \"el\", \"de\", \"y\")."
      ],
      "metadata": {
        "id": "SfYUEJTZQr02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Obtener stopwords en español\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "# Filtrar stopwords de un texto\n",
        "Texto = [\n",
        "    \"El gato está en la casa y el gato es amigo del perro.\",\n",
        "    \"El perro está en el jardín.\",\n",
        "    \"El gato y el perro son amigos.\"\n",
        "]\n",
        "\n",
        "# Tokenización por palabras\n",
        "tokens_filtrados=[]\n",
        "for i, texto in enumerate(Texto):\n",
        "  tokens = word_tokenize(Texto[i], language='spanish')\n",
        "  #print(\"Tokens:\", tokens)\n",
        "  tokens = word_tokenize(texto, language='spanish')\n",
        "  tokens_filtrados.append([word for word in tokens if word.lower() not in stop_words])\n",
        "print(\"Tokens sin stopwords:\", tokens_filtrados)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XNqeh29vQyDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77082b7f-7dfb-450e-ad4b-680944ec0e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens sin stopwords: [['gato', 'casa', 'gato', 'amigo', 'perro', '.'], ['perro', 'jardín', '.'], ['gato', 'perro', 'amigos', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.7 Stemming\n",
        "El stemming reduce las palabras a su raíz, aunque no siempre sea una palabra válida."
      ],
      "metadata": {
        "id": "yKH5RKoJQ09U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Crear un stemmer para español\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "# Aplicar stemming a una lista de palabras\n",
        "palabras = [\"corriendo\", \"correr\", \"corrió\",\"correría\",\"correcaminos\",\"corres\",\"correló\"]\n",
        "raices = [stemmer.stem(palabra) for palabra in palabras]\n",
        "print(\"Raíces:\", raices)"
      ],
      "metadata": {
        "id": "tOZgm-LxRAvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d9bd66-3fcf-4aed-8e37-8bba72b70a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raíces: ['corr', 'corr', 'corr', 'corr', 'correcamin', 'corr', 'correl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.8 Lematización\n",
        "La lematización reduce las palabras a su forma base (lema), que sí es una palabra válida."
      ],
      "metadata": {
        "id": "MJF5Ew0uRQ8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de ejemplo\n",
        "texto = \"La\"\n",
        "\n",
        "# Procesar el texto con spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Obtener los lemas\n",
        "lemas = [token.lemma_ for token in doc]\n",
        "print(\"Lemas:\", lemas)"
      ],
      "metadata": {
        "id": "-JLlI08XRYT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40173785-2edf-42a9-d5ef-8d7c3a17241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas: ['el']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.9. Part-of-Speech (POS) Tagging\n",
        "El POS tagging asigna a cada palabra una etiqueta gramatical (sustantivo, verbo, adjetivo, etc.)."
      ],
      "metadata": {
        "id": "ikmmyZ6ORDNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de ejemplo\n",
        "texto = \"El gato está durmiendo en el sofá.\"\n",
        "\n",
        "# Procesar el texto con spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Obtener las etiquetas POS\n",
        "for token in doc:\n",
        "    print(f\"Palabra: {token.text}, POS: {token.pos_}, Etiqueta: {token.tag_}\")"
      ],
      "metadata": {
        "id": "D7QGRWCTRaxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3547062b-cfaa-40a3-b6d1-06eeef71ca8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabra: El, POS: DET, Etiqueta: DET\n",
            "Palabra: gato, POS: NOUN, Etiqueta: NOUN\n",
            "Palabra: está, POS: AUX, Etiqueta: AUX\n",
            "Palabra: durmiendo, POS: VERB, Etiqueta: VERB\n",
            "Palabra: en, POS: ADP, Etiqueta: ADP\n",
            "Palabra: el, POS: DET, Etiqueta: DET\n",
            "Palabra: sofá, POS: NOUN, Etiqueta: NOUN\n",
            "Palabra: ., POS: PUNCT, Etiqueta: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.10 N-gramas\n",
        "Los n-gramas son secuencias de \"n\" palabras consecutivas. Son útiles para capturar el contexto."
      ],
      "metadata": {
        "id": "IUC77SvMRrB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"El gato está durmiendo en el sofá.\"\n",
        "\n",
        "# Tokenizar el texto\n",
        "tokens = word_tokenize(texto, language='spanish')\n",
        "\n",
        "# Generar bigramas (n=2)\n",
        "bigramas = list(ngrams(tokens, 2))\n",
        "print(\"Bigramas:\", bigramas)"
      ],
      "metadata": {
        "id": "Vk54qNoERvds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f026f8-d6c0-421c-c90f-46bc430b2491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigramas: [('El', 'gato'), ('gato', 'está'), ('está', 'durmiendo'), ('durmiendo', 'en'), ('en', 'el'), ('el', 'sofá'), ('sofá', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resumen de Conceptos\n",
        "1. **Corpus**: Colección de textos.\n",
        "\n",
        "2. **Tokenización**: Dividir texto en palabras o símbolos.\n",
        "\n",
        "3. **Normalización**: Convertir texto a un formato estándar.\n",
        "\n",
        "4. **Bag of Words**: Representación numérica de un texto.\n",
        "\n",
        "5. **Stopwords**: Palabras comunes sin significado.\n",
        "\n",
        "6. **Stemming**: Reducir palabras a su raíz.\n",
        "\n",
        "7. **Lematización**: Reducir palabras a su forma base (lema).\n",
        "\n",
        "8. **Part-of-Speech (POS)**: Etiquetado gramatical.\n",
        "\n",
        "9. **N-gramas**: Secuencias de \"n\" palabras consecutivas."
      ],
      "metadata": {
        "id": "jT9HpYfjR3iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio\n",
        "\n",
        "Crear sus propios:\n",
        "\n",
        "1. Corpus\n",
        "2. Tokenización\n",
        "3. Normalización\n",
        "4. Bag of Words\n",
        "5. Stop Words\n",
        "6. Stemming\n",
        "7. Lematización\n",
        "8. POS\n",
        "9. N-Gramas"
      ],
      "metadata": {
        "id": "MtnaMFuXHuSI"
      }
    }
  ]
}